{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'18] milestone 3: spectral graph theory\n",
    "[ntds'18]: https://github.com/mdeff/ntds_2018\n",
    "\n",
    "[Michaël Defferrard](http://deff.ch), [EPFL LTS2](https://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Students\n",
    "\n",
    "* Team: 6\n",
    "* Students: Gabor Csordas, Maëlle Le Clainche, Nicolas Fontbonne, Marie Sadler\n",
    "* Dataset: Flights routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Rules\n",
    "\n",
    "* Milestones have to be completed by teams. No collaboration between teams is allowed.\n",
    "* Textual answers shall be short. Typically one to two sentences.\n",
    "* Code has to be clean.\n",
    "* You cannot import any other library than we imported.\n",
    "* When submitting, the notebook is executed and the results are stored. I.e., if you open the notebook again it should show numerical results and plots. We won't be able to execute your notebooks.\n",
    "* The notebook is re-executed from a blank state before submission. That is to be sure it is reproducible. You can click \"Kernel\" then \"Restart & Run All\" in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objective\n",
    "\n",
    "The goal of this milestone is to get familiar with the graph Laplacian and its spectral decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0 Load your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you get a `No module named 'sklearn'` error when running the below cell, install [scikit-learn](https://scikit-learn.org) with `conda install scikit-learn` (after activating the `ntds_2018` environment).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy import linalg\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('seaborn')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "# for ground truth checking\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's denote your graph as $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, A)$, where $\\mathcal{V}$ is the set of nodes, $\\mathcal{E}$ is the set of edges, $A \\in \\mathbb{R}^{N \\times N}$ is the (weighted) adjacency matrix, and $N = |\\mathcal{V}|$ is the number of nodes.*\n",
    "\n",
    "*Import the adjacency matrix $A$ that you constructed in the first milestone.\n",
    "(You're allowed to update it between milestones if you want to.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency =  np.load(\"adjacency.npy\")\n",
    "adjacency_unweighted = np.copy(adjacency)\n",
    "adjacency_unweighted[adjacency_unweighted!=0]=1\n",
    "degrees = np.sum(adjacency_unweighted, axis = 0)\n",
    "n_nodes =  adjacency_unweighted.shape[0]\n",
    "## We are removing those edges where the weight is smaller thane the threshold\n",
    "threshold = 20\n",
    "node_map = np.where(degrees >= threshold)[0]\n",
    "adjacency_th = np.delete(adjacency_unweighted,np.where(degrees < threshold),0)\n",
    "adjacency_th = np.delete(adjacency_th,np.where(degrees < threshold),1)\n",
    "degrees_th = np.sum(adjacency_th, axis = 0)\n",
    "n_nodes_th = adjacency_th.shape[0]\n",
    "\n",
    "adjacency_csr = sparse.csr_matrix(adjacency_unweighted);\n",
    "degree_matrix_csc = sparse.diags(degrees,format = \"csc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 Graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 1\n",
    "\n",
    "*From the (weighted) adjacency matrix $A$, compute both the combinatorial (also called unnormalized) and the normalized graph Laplacian matrices.*\n",
    "\n",
    "*Note: if your graph is weighted, use the weighted adjacency matrix. If not, use the binary adjacency matrix.*\n",
    "\n",
    "*For efficient storage and computation, store these sparse matrices in a [compressed sparse row (CSR) format](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_.28CSR.2C_CRS_or_Yale_format.29).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_combinatorial_csr =  sparse.csr_matrix(degree_matrix_csc - adjacency_csr);\n",
    "inv_degree_matrix_csr = sparse.linalg.inv(degree_matrix_csc).tocsr()\n",
    "sqrt_inv_degree_matrix_csr = sparse.csr_matrix.sqrt(inv_degree_matrix_csr)\n",
    "laplacian_normalized_csr = sqrt_inv_degree_matrix_csr * laplacian_combinatorial_csr * sqrt_inv_degree_matrix_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Use one of them as the graph Laplacian $L$ for the rest of the milestone.\n",
    "We however encourage you to run the code with both to get a sense of the difference!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = laplacian_normalized_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2\n",
    "\n",
    "*Compute the eigendecomposition of the Laplacian $L = U^\\top \\Lambda U$, where the columns $u_k \\in \\mathbb{R}^N$ of $U = [u_1, \\dots, u_N] \\in \\mathbb{R}^{N \\times N}$ are the eigenvectors and the diagonal elements $\\lambda_k = \\Lambda_{kk}$ are the corresponding eigenvalues.*\n",
    "\n",
    "*Make sure that the eigenvalues are ordered, i.e., $0 = \\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_N$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate eigenvalues and eigenvectors\n",
    "[eigenvalues, eigenvectors] = sparse.linalg.eigsh(laplacian_normalized_csr, k = n_nodes-1, which = 'LM')\n",
    "\n",
    "#This function will not return the first eigenvalue 0 because we know that the first eigenvalue is always 0\n",
    "#So when we now look at eigenvalues we should not forget that there is one more 0.\n",
    "\n",
    "# sort the resulting values and vectors\n",
    "sortID = np.argsort(eigenvalues)\n",
    "eigenvalues = eigenvalues[sortID]\n",
    "eigenvalues[eigenvalues < 10**(-10)] = 0\n",
    "\n",
    "eigenvectors = eigenvectors[:,sortID]\n",
    "eigenvectors[eigenvectors < 10**(-10)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Justify your choice of eigensolver.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "\n",
    "The Laplacian is always a symmetric real valued matrix and therefore a Hermitian matrix as well. So we can use the solver for sparse Hermitian matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 3\n",
    "\n",
    "* *We can write $L = S S^\\top$. What is the matrix $S$? What does $S^\\top x$, with $x \\in \\mathbb{R}^N$, compute?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "The matrix S is the incidence matrix having as rows the nodes and as columns the edges ($S \\in \\mathbb{R}^{NxM}$, N the number of nodes and M the number of edges). For an unweighted graph, $S_{i,j}$ = 1, for an edge $e_j$ between two vertexes $v_{i,k}$, -1 for $e_j$ between $v_{k,i}$ and 0 otherwise (the choice of sign is arbitrary, but it has to be once positive and once negative for a given edge). For a weighted graph, the entries are $\\sqrt{w_{i,k}}$ and $-\\sqrt{w_{i,k}}$, instead of +1 and -1, respectively. \n",
    "\n",
    "$S^\\top$ acts as a graph gradient, and $S^\\top x$ for a weighted graph computes:\n",
    "\n",
    "\\begin{equation}\n",
    "(S^\\top x )[j]= \\sqrt{w_{i,k}} (x[i] - x[k])\n",
    "\\end{equation}\n",
    "\n",
    "which corresponds to the derivative of x along edge j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4\n",
    "\n",
    "* *Show that $\\lambda_k = \\| S^\\top u_k \\|_2^2$, where $\\| \\cdot \\|_2^2$ denotes the squared Euclidean norm (a.k.a. squared $L^2$ norm).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\lambda_k & = u_k^\\top L u_k \\text{, result of the eigendecomposition of $L$ with $u_k$ being a unit-vector}\\\\ \n",
    " & = u_k^\\top S S^\\top  u_k \\text{      , with $L = S S^\\top$}\\\\\n",
    " & = (S^\\top u_k)^\\top S^\\top  u_k \\text{        , where the order of factors reverses when taking the transpose}\\\\\n",
    " & =\\| S^\\top u_k \\|_2^2 \\text{     , with $x^\\top x$ being the squared Euclidean norm}\\\\\n",
    "\\end{split}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *What does the quantity $\\| S^\\top x \\|_2^2$ tell us about $x$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "This quantity tells us how \"smooth\" x is, i.e. a larger quantity means a higher variation among the x vector components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 5\n",
    "\n",
    "* *What is the value of $u_0$, both for the combinatorial and normalized Laplacians?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    " * **Combinatorial Laplacian**\n",
    "\n",
    "$u_0$ corresponds to the eigenvalue 0 and cannot be the vector 0. From the definition of the eigendecomposition we have:\n",
    "\n",
    "\\begin{equation}\n",
    "L u_0 = \\lambda_0 u_0 = 0 \n",
    "\\end{equation}\n",
    "\n",
    "Multiply by $u_0^\\top$ gives: \n",
    "\\begin{equation}\n",
    "u_0^\\top L u_0 = u_0^\\top \\cdot 0 = 0\n",
    "\\end{equation}\n",
    "\n",
    "Since $u_0^\\top L u_0$ is defined as follows:\n",
    "\\begin{equation}\n",
    "u_0^\\top L u_0 = \\frac{1}{2} \\sum_{\\substack{u,v} \\in E} w_{u,v}(u_0[u]-u_0[v])^2\n",
    "\\end{equation}\n",
    "\n",
    "We get: \n",
    "\\begin{equation}\n",
    "\\sum_{\\substack{u,v} \\in E} w_{u,v}(u_0[u]-u_0[v])^2 = 0 \n",
    "\\end{equation}\n",
    "\n",
    "For this equation to hold, we need to have $u_0[u] = u_0[v]$ for any edge (u,v) $\\in E$. In the case where the graph is connected, we get that $u_0[i] = u_0[k]$ for every i,k $\\in V$. From this we get that $u_0$ equals: \n",
    "\\begin{align}\n",
    "    u_0 &= \\alpha \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           1 \\\\\n",
    "           \\vdots \\\\\n",
    "           1\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}\n",
    "\n",
    "with $\\alpha \\in \\mathbb{R}^*$, or to have a unit vector, $\\alpha$ needs to equal $\\alpha = \\frac{1}{\\sqrt{N}}$, where $N$ is the number of connected nodes. Thus, the value of $u_0$ is the unit vector $e$. \n",
    "\n",
    "- **Normalized Laplacian**\n",
    "\n",
    "Let's call $u_0'$ the eigenvector of the normalized Laplacian $L_{norm}$.\n",
    "\n",
    "From the theory, we know that $u_0' = D^{\\frac{1}{2}} u_0$, and hence $u_0' =  D^{\\frac{1}{2}} e$ is the eigenvector of $L_{norm}$ of eigenvalue 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 6\n",
    "\n",
    "- *Look at the spectrum of the Laplacian by plotting the eigenvalues.\n",
    "Comment on what you observe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eigenvalues.real, \"b.\" , markersize = 1)\n",
    "plt.xlabel('Eigenvalue Index')\n",
    "plt.ylabel('Eigenvalue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "As espected the eigenvalues are in the domain $[0,2]$. The first eigenvalues which equal to 0 represent the number of connected components. \n",
    "\n",
    "It is interesting to observe that a lot of eigenvalues equal 1. In this notebook we will not further investigate into this property, but we will keep it in mind for the final report. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *How many connected components are there in your graph? Answer using the eigenvalues only.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in addition to the 0-multiplicity we add 1 which corresponds to the first eigenvalue 0 which is omitted in the computation\n",
    "n_conn_comp = eigenvalues[eigenvalues == 0].shape[0] + 1 \n",
    "print(\"Number of connected components:\",n_conn_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **8** connected components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Is there an upper bound on the eigenvalues, i.e., what is the largest possible eigenvalue? Answer for both the combinatorial and normalized Laplacians.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "For normalized Laplacians the upper bound on the eigenvalues is 2, where equality holds iff the graph is bipartite. \n",
    "\n",
    "For our graph (normalized graph and the threshold graph), we observed that the upper bound of the eigenvalues of the combinatorial Laplacian was the same than the maximum degree. Defining the upper bound is an active field of research, and diverse theorems of tight bounds can be found in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 Laplacian eigenmaps\n",
    "\n",
    "*Laplacian eigenmaps* is a method to embed a graph $\\mathcal{G}$ in a $d$-dimensional Euclidean space.\n",
    "That is, it associates a vector $z_i \\in \\mathbb{R}^d$ to every node $v_i \\in \\mathcal{V}$.\n",
    "The graph $\\mathcal{G}$ is thus embedded as $Z \\in \\mathbb{R}^{N \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 7\n",
    "\n",
    "* *What do we use Laplacian eigenmaps for? (Or more generally, graph embeddings.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "We use Laplacian eigenmaps for **dimensionality reduction**. Often, graph data is intrinsically low dimensional, but lies in a very high-dimensional space. Thus, **mapping a network to a vector space and reducing the dimension, while preserving relevant graph properties**, can be useful for faster **computation, machine learning algorithms, statistics, or visualization**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 8\n",
    "\n",
    "*Embed your graph in $d=2$ dimensions with Laplacian eigenmaps.\n",
    "Try with and without re-normalizing the eigenvectors by the degrees, then keep the one your prefer.*\n",
    "\n",
    "*Recompute the eigenvectors you need with a partial eigendecomposition method for sparse matrices.\n",
    "When $k \\ll N$ eigenvectors are needed, partial eigendecompositions are much more efficient than complete eigendecompositions.\n",
    "A partial eigendecomposition scales as $\\Omega(k |\\mathcal{E}|$), while a complete eigendecomposition costs $\\mathcal{O}(N^3)$ operations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the matrix in the good format\n",
    "adjacency_lc_csr = sparse.csr_matrix(adjacency_th);\n",
    "degrees_lc = np.sum(adjacency_th, axis = 0)\n",
    "degree_matrix_lc_csc = sparse.diags(degrees_lc,format = \"csc\")\n",
    "n_nodes_lc =  degrees_lc.shape[0]\n",
    "\n",
    "#Computation of the laplacian for our graph\n",
    "laplacian_combinatorial_lc_csr =  sparse.csr_matrix(degree_matrix_lc_csc - adjacency_lc_csr);\n",
    "inv_degree_matrix_lc_csr = sparse.linalg.inv(degree_matrix_lc_csc).tocsr()\n",
    "sqrt_inv_degree_matrix_lc_csr = sparse.csr_matrix.sqrt(inv_degree_matrix_lc_csr)\n",
    "laplacian_normalized_lc_csr = sqrt_inv_degree_matrix_lc_csr * laplacian_combinatorial_lc_csr * sqrt_inv_degree_matrix_lc_csr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_embedding(laplacian_normalized_csr, d):\n",
    "    [eigenvalues, eigenvectors] = sparse.linalg.eigsh(laplacian_normalized_csr, k = d+1, which = 'SM')\n",
    "    sortID = np.argsort(eigenvalues)\n",
    "    eigenvalues = eigenvalues[sortID]\n",
    "    eigenvectors = eigenvectors[:,sortID]\n",
    "    proj  = eigenvectors[:,1:d+1]\n",
    "    return proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Plot the nodes embedded in 2D. Comment on what you see.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = graph_embedding(laplacian_normalized_lc_csr,  2)\n",
    "plt.plot(proj[:,0],proj[:,1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "With the embedding in two dimensions, airports that are connected by a flight now appear close to each other in the plot. We start to see clusters that naturally emerge form the geometry. Since the clusters represent airports that are well connected to each other, there might be another meaning to it such as financially, economically, and culturally similar countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the embedding $Z \\in \\mathbb{R}^{N \\times d}$ preserve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "The embedding $Z \\in \\mathbb{R}^{N \\times d}$ preserves relevant graphs properties such as the number of data points N, while reducing the dimension d. \n",
    "The goal of the Laplacian eigenmap algorithm is to preserve local information optimally. All the point node that are close to each other, meaning nodes connected by an edge with a large weight, must remain close to each other in the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 Spectral clustering\n",
    "\n",
    "*Spectral clustering is a method to partition a graph into distinct clusters.\n",
    "The method associates a feature vector $z_i \\in \\mathbb{R}^d$ to every node $v_i \\in \\mathcal{V}$, then runs [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering) in the embedding space $\\mathbb{R}^d$ to assign each node $v_i \\in \\mathcal{V}$ to a cluster $c_j \\in \\mathcal{C}$, where $k = |\\mathcal{C}|$ is the number of desired clusters.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we have chosen to use the unweighted adjacency matrix, because we feel that our graph will probably be clustered according to geographical features. Moreover we kept only the nodes with a degree larger than 20, because we want to see only the airports (=nodes) that are significant (an airport with less than 20 flights is not very significant). By applying this threshold, we also assure that our graph is connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of the eigenvalues and eigenvectors for our graph\n",
    "[eigenvalues_lc, eigenvectors_lc] = sparse.linalg.eigsh(laplacian_normalized_lc_csr, k = n_nodes_lc-1, which = 'LM')\n",
    "#This function will not return the first 0 because it is assuming it is always there. \n",
    "#So when we now look at eigenvalues we had not to forget that there is one more 0.\n",
    "sortID = np.argsort(eigenvalues_lc)\n",
    "eigenvalues_lc = eigenvalues_lc[sortID]\n",
    "eigenvectors_lc = eigenvectors_lc[:,sortID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conn_comp = eigenvalues_lc[eigenvalues_lc ==  0].shape[0]+1\n",
    "eigenvalues_lc[eigenvalues_lc < 10**(-10)] = 0\n",
    "print(\"Number of connected components:\",n_conn_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Spectrum\n",
    "plt.plot(eigenvalues_lc, \"b.\" , markersize = 1)\n",
    "plt.title(\"Graph Spectrum\")\n",
    "plt.xlabel(\"Numbers of the eigenvalues\")\n",
    "plt.ylabel(\"Values of eigenvalues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 10\n",
    "\n",
    "* *Choose $k$ and $d$. How did you get to those numbers?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "- k is chosen according to the graph of eigenvalues : the number of eighenvalues before the \"gap\" corresponds to k. k is the number of clusters.\n",
    "- d is the dimension of the embedding space, it has to be chosen in order to preserve local infomation optimally in a certain sense. (after Belkin's book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot eigenvalues repartitions\n",
    "x = np.ones_like(range(len(eigenvalues_lc)))\n",
    "plt.plot(eigenvalues_lc,x,\"b.\",markersize = 10)\n",
    "plt.xlim([0,0.5])\n",
    "plt.title(\"Repartition of eigenvalues (zoom on the part next to 0)\")\n",
    "plt.xlabel(\"Value of the eigenvalue\")\n",
    "\n",
    "##count how many values we have until the gap\n",
    "nb_before_1rst=len(eigenvalues_lc[np.where(eigenvalues_lc< 0.2)])+1 # +1 correspond to the first 0 not in the list of eigenvalues\n",
    "nb_before_2nd=len(eigenvalues_lc[np.where(eigenvalues_lc< 0.4)])+1  # +1 correspond to the first 0 not in the list of eigenvalues\n",
    "print(\"number of eigenvalues before 1st gap :\",nb_before_1rst)\n",
    "print(\"number of eigenvalues before 2nd gap :\",nb_before_2nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we see on the graph, we have two big gaps. The first corresponds to k=3 and the second to k=6. We will test these two possibilities\n",
    "- We have decided to take d=k, because d is the number of eigenvectors to keep when applying the K-means. If d<k, the number of information may be to little to process a good clustering. However if d is large, the computational cost will be higher. Considering these facts, d=k seems a good compromise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 11\n",
    "\n",
    "1. *Embed your graph in $\\mathbb{R}^d$ as $Z \\in \\mathbb{R}^{N \\times d}$.\n",
    "   Try with and without re-normalizing the eigenvectors by the degrees, then keep the one your prefer.*\n",
    "1. *If you want $k=2$ clusters, partition with the Fiedler vector. For $k > 2$ clusters, run $k$-means on $Z$. Don't implement $k$-means, use the `KMeans` class imported from scikit-learn.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to do the embedding with the eigenvectors obtained from the eigendecomposition of the normalized Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For k=3 and d=3\n",
    "k = 3; d = 3\n",
    "H = eigenvectors_lc[:,:d]; \n",
    "clusters3 = KMeans(n_clusters=k, random_state=0).fit_predict(H)\n",
    "\n",
    "print(\"----- For k=\",k,\" and d=\",d,\" -----\")\n",
    "print(\"Number of elements in clusters :\")\n",
    "for i in range(k):\n",
    "    cnt = 0\n",
    "    for j in clusters3:\n",
    "        if j == i:\n",
    "            cnt +=1\n",
    "    print(\"Cluster \",i+1,\":\",cnt)\n",
    "\n",
    "#For k=6 and d=6\n",
    "k = 6; d = 6\n",
    "H = eigenvectors_lc[:,:d]; \n",
    "clusters6 = KMeans(n_clusters=k, random_state=0).fit_predict(H)\n",
    "print()\n",
    "print(\"----- For k=\",k,\" and d=\",d,\" -----\")\n",
    "print(\"Number of elements in clusters :\")\n",
    "for i in range(k):\n",
    "    cnt = 0\n",
    "    for j in clusters6:\n",
    "        if j == i:\n",
    "            cnt +=1\n",
    "    print(\"Cluster \",i+1,\":\",cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For k=2, we have :\n",
    "fiedler_vect = np.sign(eigenvectors_lc[:,0])\n",
    "nb_neg=len(fiedler_vect[np.where(fiedler_vect==-1)])\n",
    "nb_pos=len(fiedler_vect[np.where(fiedler_vect==1)])\n",
    "print(\"----- For k=2 (this is just an exemple, it has no real sense for our graph) -----\")\n",
    "print(\"Number of elements in clusters :\")\n",
    "print(\"Cluster labeled +1 :\",nb_pos)\n",
    "print(\"Cluster labeled -1 :\",nb_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 12\n",
    "\n",
    "- *Use the computed cluster assignment to reorder the adjacency matrix $A$.\n",
    "What do you expect? What do you observe?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For k=3\n",
    "new_order3 = np.array([],dtype = int)\n",
    "for i in range(3):\n",
    "    new_order3 = np.append(new_order3,np.where(clusters3 == i))\n",
    "plt.spy(adjacency_th[:,new_order3][new_order3], markersize=1)\n",
    "plt.title(\"Reordered ajacency matrix for k=3\")\n",
    "\n",
    "#For k=6\n",
    "plt.figure()\n",
    "new_order6 = np.array([],dtype = int)\n",
    "for i in range(6):\n",
    "    new_order6 = np.append(new_order6,np.where(clusters6 == i))\n",
    "plt.spy(adjacency_th[:,new_order6][new_order6], markersize=1)\n",
    "plt.title(\"Reordered ajacency matrix for k=6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were expecting a block diagonal matrix or something close to it, because the nodes of one cluster are supposed to be mostly connected with the nodes of this same cluster. Of course the clusters are not fully independent (in this case, our graph would have several distinct components).\n",
    "- For k=3, we observe 3 blocks in the diagonal. However, those blocks seems to have \"internal\" blocks : that means that it may exist some \"internal\" clusters in clusters. This is fully visible in the case k=6.\n",
    "- For k=6, the diagonal is composed of 6 blocks. Some blocks (like the 3rd - 167 nodes) are much bigger than others (like the 2nd - 11 nodes). The blocks are still connected with the other blocks even if they are mostly link with themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 13\n",
    "\n",
    "*If you have ground truth clusters for your dataset, compare the cluster assignment from spectral clustering to the ground truth.\n",
    "A simple quantitative measure is to compute the percentage of nodes that have been correctly categorized.\n",
    "If you don't have a ground truth, qualitatively assess the quality of the clustering.*\n",
    "\n",
    "*Ground truth clusters are the \"real clusters\".\n",
    "For example, the genre of musical tracks in FMA, the category of Wikipedia articles, the spammer status of individuals, etc.\n",
    "Look for the `labels` in the [dataset descriptions](https://github.com/mdeff/ntds_2018/tree/master/projects/README.md).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no labels in our dataset, we have chosen to check our hypothesis of geographical clusters. In order to do that we import back our dataframe of airports and flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of routes\n",
    "routes = pd.read_csv('routes.dat', sep=',', encoding='utf-8', engine='python')\n",
    "routes.columns = ['Airline','Airline ID','Source Airport','Source Airport ID','Destination Airport','Destination Airport ID','Codeshare','Stops','Equipment']\n",
    "routes = routes.drop(columns=['Source Airport ID','Destination Airport ID'])\n",
    "\n",
    "# import of source and destination airport\n",
    "source_airports = routes[['Source Airport']]\n",
    "source_airports = source_airports.rename(columns={'Source Airport':'Airport'})\n",
    "\n",
    "dest_airports = routes[['Destination Airport']]\n",
    "dest_airports = dest_airports.rename(columns={'Destination Airport':'Airport'})\n",
    "\n",
    "# creation of a dataframe with all airport and airport_idx \n",
    "# (we use airport_idx insteed of airportID because some airports have no airportID)\n",
    "airports = pd.concat([source_airports,dest_airports])\n",
    "airports = airports.drop_duplicates()\n",
    "airports.reset_index(inplace=True)\n",
    "airports = airports.drop(columns=['index'])\n",
    "airports.reset_index(inplace=True)\n",
    "airports = airports.set_index('Airport')\n",
    "airports = airports.rename(columns={'index':'airport_idx'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For clustering with k=3\n",
    "print(\"--------------------------------- For k=3 ---------------------------------\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Cluster\",i+1,\" :\\n\",airports.index[node_map[np.where(clusters3 == i)]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For k=3, we observe that our cluster are relative to the continents.\n",
    "- **Cluster 1:** European Airports (ex : Geneva, Lyon, Dublin, Rome, Vienne etc) + some Indonesian Airports (Jawa Timur, Jakarta)\n",
    "- **Cluster 2:** American Continent Airport (ex : Peru, America, Canada etc)\n",
    "- **Cluster 3:** Asian Continent (ex : China, Japan)\n",
    "\n",
    "We notice that some airports are badly clustered if we consider geographical clusters : for exemple ITM (Osaka Airport, Japan) is in Cluster 1 (\"Europe\"). Moreover the \"mix\" of geographic areas in cluster 1 really shows that some in-clusters exist. Choosing a higher k will probably show a result closer to the geographical \"reality\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For clustering with k=6\n",
    "print(\"--------------------------------- For k=6 ---------------------------------\\n\")\n",
    "\n",
    "for i in range(6):\n",
    "    print(\"Cluster\",i+1,\" :\\n\",airports.index[node_map[np.where(clusters6 == i)]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For k=6, we observe that our clusters are more relative to smaller areas (countries or region of countries).\n",
    "- **Cluster 1:** North and Central America (Peru, Colombia, and a lot of cities of the USA)\n",
    "- **Cluster 2:** South Ameria (Bresil, Argentine)\n",
    "- **Cluster 3:** Europe (France, Switzerland, Italy, UK, etc)\n",
    "- **Cluster 4:** Asia and Indonesia (Phillipines, Thailande, Chine, Japon, etc)\n",
    "- **Cluster 5:** Africa (Cote d'Ivoire, Egypt, Arabie Saoudite, Iran, etc)\n",
    "- **Cluster 6:** Russia (+ some countries linked to Russia like Kazakhstan)\n",
    "\n",
    "This clustering is really qualitative and shows some meaningful relations between countries. The case of Russia is quite revealing : it is a separate cluster and includes a lot of countries from the former Soviet Union."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 14\n",
    "\n",
    "Plot the cluster assignment (one color per cluster) on the 2D embedding you computed above with Laplacian eigenmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['Blue', 'Red', 'Green']\n",
    "\n",
    "for i in range(3):\n",
    "    cluster = proj[np.where(clusters3 == i)[0], :].real\n",
    "    plt.plot(cluster[:,0],cluster[:,1], '.', color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "- **Blue :** Europe + Indonesia (Jawa Timur, Jakarta)\n",
    "- **Red :** America\n",
    "- **Green :** Asia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 15\n",
    "\n",
    "Why did we use the eigenvectors of the graph Laplacian as features? Could we use other features for clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "The eigenvectors give the embedding of the node in a lower dimensional space keeping similar nodes (in the sense of their egde weigth) close together. Similar nodes are close to each other considering the $L-$distance used for the $k$-mean clustering. Therefore, the eigenvectors are good choice for clustering nodes. \n",
    "\n",
    "We could have used other features, such as the eigenvectors of the adjacency matrix. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
